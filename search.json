[{"title":"我推荐你在 Windows 10 上安装这些应用","url":"/articles/2020/11/13/interesting-things/apps-on-windows/","content":"\n这些程序中, 有很多都是我用了几年的, 它们都很完美. 其中不乏个人开发者开发和更新维护的软件, 希望大家用的时候如果觉得好用, 可以推荐给其他人.\n\n对于高效工作和生活来说, 这些是必备程序:\n\n- [Everything](https://www.voidtools.com/downloads/)\n- [Snipaste](https://zh.snipaste.com/download.html)\n- [Myersplash](https://juniperphoton.net/myersplash/)\n- [Firefox](https://www.mozilla.org/en-US/firefox/new/)\n- [Microsoft To-Do](https://to-do.microsoft.com/tasks/)\n- [Simple Sticky Notes](https://www.simplestickynotes.com/)\n- [PowerToys](https://github.com/microsoft/PowerToys/releases/)\n- [OneNote](https://www.onenote.com/download)\n- [Remotely](https://app.remotely.one/Agents)\n\n还有一些普通人可能用不上的:\n\n- [chocolatey](https://chocolatey.org/install)\n- [Keycastow](https://chocolatey.org/packages/keycastow)\n- [OBS](https://obsproject.com/download)\n- [FFMpeg](https://ffmpeg.org/download.html)\n\n一些普通人几乎不知道的:\n\n- [ClevoECView](https://github.com/SkyLandTW/ClevoECView)\n"},{"title":"Redis 数据类型及应用","url":"/articles/2020/11/12/database/redis-points/","content":"\n## 数据类型的比较\n\n| 类型   | 底层原理 | 支持的数据操作 | 设置 | 应用场景 |\n| ------ | -------- | -------------- | ---- | -------- |\n| key    |          |                |      |          |\n| string |          |                |      |          |\n|        |          |                |      |          |\n|        |          |                |      |          |\n|        |          |                |      |          |\n|        |          |                |      |          |\n|        |          |                |      |          |\n|        |          |                |      |          |\n|        |          |                |      |          |\n\n## 一些典型的场景及数据类型的高级应用\n\n## 与其他内存数据库的对比\n"},{"title":"设计健壮和幂等的 Web API","url":"/articles/2020/11/10/infrastructure/design-robust-web-api/","content":"\n网络和服务端的问题都会导致 API 出错, 解决这个问题需要遵循 3 个原则:\n\n1. 网络故障后, 客户端需要重新发送请求来保持一致性\n2. 客户端因为网络故障重新发送的请求需要保持幂等性, 也就是说需要带上一个用于保持幂等的 id\n   1. 在 RESTful API 中, PUT, GET, DELETE 这些动词都具有幂等性\n   2. POST 不能保持幂等, 因此需要使用一个键 id 来区分请求. 这个键是这样用的:\n      1. 如果客户端这一侧在发请求前失败了, 重发后对于服务端来说是透明的, 不存在什么问题\n      2. 如果客户端发请求到服务端后, 服务端处理时出错了, 那么需要数据库具备 ACID 特性, 将处理包装成事务即可. 失败了就标记该 id 并返回错误.\n      3. 如果服务端事务处理成功了, 但响应没能到达客户端, 那么客户端会重发请求, 服务端只需要根据 id 将缓存的结果返回即可\n3. 同时, 需要避免客户端重新发送请求的频率, 避免请求阻塞.\n"},{"title":"利用 JSON Schema 来保障数据流中数据的完整和一致性","url":"/articles/2020/11/03/infrastructure/sharing-json-schema/","content":"\nJSON Schema 是一个比较简单的数据定义规范, 可以先看看官方的文档: [What is a schema](https://json-schema.org/understanding-json-schema/about.html). 利用 JSON Schema 我们可以在项目需求分析阶段确定数据格式, 方便统一文档, 也可以在具体项目中用于数据有效性验证, 确保整个数据流中数据的完整和一致.\n\n## 设计\n\n设计 JSON schema 有点类似于设计 SQL 数据库中的数据表, 设计 SQL 表时需要遵循 3NF, 而在设计 JSON Schema 时需要遵循这些原则:\n\n1. 可组织. 将一个一个小的 schema 模块组织起来获得更复杂的数据结构.\n2. 可扩展.\n3. 可维护.\n\n我们可以借助一些工具来定义 Model 类, 然后通过类的继承和扩展实现 Schema 的组织/扩展. 比如 Python 中有 pydantic 包可以很方便的将 Model 导出成 JSON Schema.\n\n## 管理和应用\n\n在存储 Schema 时也不必用 JSON 格式, JSON, TOML, YAML 甚至 python 代码都是可以相互翻译转换的, 尽量使用适合已有系统的, 更可读的方式存储 Schema.\n\n将 JSON schema 发布到注册表服务并分配一个 id, 并在数据发送时带上 id, 在数据消费时通过 schema id 进行数据校验. 通过 NACOS 可以很方便地实现这个功能:\n\n![json schema](https://latina-1253549750.cos.ap-shanghai.myqcloud.com/essay/imgs/20201103100021.png)\n\n也可以通过其他的数据库进行存储, 比如 redis/MySQL/mongoDB 等等.\n\n## 实践\n\n1. 通过 python 代码定义 model\n2. 通过 model 动态生成 schema\n3. 在 CI/CD 中部署 schema 到 NACOS\n4. 在 schema 分组时, 可以参考 redis 的键的设置.\n5. 在发送数据时在数据中带上 schema id\n6. 在消费数据时根据 schema id 验证数据\n"},{"title":"Weekly Update, 10 27, 20","url":"/articles/2020/10/27/weekly-update/20201027/","content":"\n### Work\n\n- Fix performance issues when using python I/O.\n\n### TWIL\n\nThis week I learn:\n\n- tmux & vim\n- Clojure concurrency model\n\n### To be done in next week\n\n- Golang in action\n- LeetCode 10\n- Dissertation researc proposal\n\n### Things Can Be Improved\n\n- More sports, less sugar.\n"},{"title":"Weekly Update, 10 16, 20","url":"/articles/2020/10/16/weekly-update/20201016/","content":"\n### Work\n\n- A python project, watch DBF files then push data to Kafka.\n  - Existing DBF packages cannot handle column missing issue thus will extract wrong data. I work around this by reading bytes according to pre-defined rules.\n\n### TWIL\n\nThis week I learn:\n\n- I read {seven concurrency models in seven weeks}\n\n### To be done in next week\n\n- LeetCode\n- Read source code of `asyncio`\n\n### Things Can Be Improved\n\n- Less LOL\n- More sleep\n"},{"title":"Weekly Update, 09 27, 20","url":"/articles/2020/09/27/weekly-update/200927/","content":"\nThis is the 1st post in category `weekly-update`, my goal is to become financial dependent, hope I can scrum it successfully!\n\n### Work\n\n- A study and test of Level-2 quote api of A-Share market:\n  - Tonghuashun\n  - East Money\n  - Sina\n  - CSMA\n  - etc.\n- A study and test of real time transaction api of A-Share market:\n  - CATS\n\n### To be done in next week\n\n- Establish an private NTP server\n- A project to get real time account information and positions\n- Dive deep into concurrent programming with book `Seven Concurrency Models in Seven Weeks`\n- Re-Read book `Introduction to Algorithms`\n\n### Things Can Be Improved\n\n- Spend more time on books\n"},{"title":"Ansible： 老项目 Devops 救星","url":"/articles/2020/07/10/cicd/ansible-in-action/","content":"\n注：本文首发于`武大NLP实验室`公众号，ID：`WHU_NLP`，欢迎关注！\n\n## 痛点\n\n实验室服务器数量的增多让基础设施变得更加复杂，当需要更新一些实验和项目所依赖的基础环境时的你：\n\n![环境配置1](https://latina-1253549750.cos.ap-shanghai.myqcloud.com/essay/imgs/20200702103555.png)\n\n此外，横跨几届的老项目的部署和维护已经成为了开发人员的噩梦，如果文档不健全，在更新部署时可能需要一天的时间，而文档健全时的情况会好一些，只需要 24 个小时：\n\n<img src=\"https://latina-1253549750.cos.ap-shanghai.myqcloud.com/essay/imgs/20200702002712.png\" alt=\"环境管理\" style=\"zoom:70%;\" />\n\n并且，在多人共享服务器时，权限控制也很麻烦。为了避免某位有 root 权限的用户的误操作如清理了某些目录或者停止了某些核心进程而导致服务器运行异常，管理员往往只给用户普通权限，而这样一刀切的处理方式又会使管理员频繁地因为 root 权限申请被打扰。\n\n![权限管理](https://latina-1253549750.cos.ap-shanghai.myqcloud.com/essay/imgs/20200702002732.png)\n\n## 可能的解决方法\n\n那么有什么办法来解决上面的这些问题呢？手动运行命令可能是对基础设施进行维护最直接的方法，但效率实在太低，而且失误的概率较大。\n\n![image-20200702105941121](https://latina-1253549750.cos.ap-shanghai.myqcloud.com/essay/imgs/20200702105941.png)\n\n针对具体任务编写小脚本似乎会效率更高，但调试的过程可能会比较痛苦，扩展性、安全性、可读性均比较差，而且运行记录难以追踪、运行后难以回溯。\n\n![image-20200702003352104](https://latina-1253549750.cos.ap-shanghai.myqcloud.com/essay/imgs/20200702003352.png)\n\n那么有没有一种简单解决上述问题的方法呢？\n\n## 基础设施/架构即代码(Infrastructure as Code)\n\n![Infrastructure as code defines the environment in a versioned file](https://latina-1253549750.cos.ap-shanghai.myqcloud.com/essay/imgs/20200703111856.png)\n\nInfrastructure as Code 简称 IaC，是 DevOps 的一项重要实践，具体就是用一种描述性的(declarative)模型来管理基础设施，比如网络、服务器、虚拟机、负载平衡器及他们的拓扑结构等资源。IaC 的重要原则是幂等性，对于一套固定的基础设施，相同的描述性的配置总会让基础设施进入到一个相同的状态。这种幂等性让基础设施的状态可复制、可扩展、可追踪并且可回溯。IaC 通常和持续交付结合使用。\n\nIaC 实践中通常使用一种描述性的编码语言，如 JSON, YAML, XML 等，这些语言大多简单易学，且具备良好的可读性。下面这段 YAML 代码所描述的内容和 MarkDown 十分类似，相信不是程序员也能理解其含义。\n\n```yaml\n# Employee records\n- martin:\n    name: Martin D'vloper\n    job: Developer\n    skills:\n      - python\n      - perl\n      - pascal\n- tabitha:\n    name: Tabitha Bitumen\n    job: Developer\n    skills:\n      - lisp\n      - fortran\n      - erlang\n```\n\n### 可用方案对比\n\nIaC 发展已久，目前有许多可用方案。\n\n![IaC 方案对比](https://latina-1253549750.cos.ap-shanghai.myqcloud.com/essay/imgs/20200703115355.png)\n\n通过从易用性、扩展性和价格等维度对比已有的 IaC 方案，我们发现 Ansible 是一个比较好的选择。除此之外，Ansible 相比于其他的项目，还有一项最重要的特点，在于 Agentless，即无客户端/无代理的运行模式。\n\n### Ansible：开源的基础设施即代码解决方案\n\n![image-20200703120008312](https://latina-1253549750.cos.ap-shanghai.myqcloud.com/essay/imgs/20200703120008.png)\n\nAnsible 的第一个版本发布于 2012 年 2 月，至今已经迭代了很长时间。\n\nAnsible 基于纯文本 YAML 文件来描述资源的行为、状态或者特定任务，几乎所有地方都支持设置变量，包括 playbook、文件、inventory、命令、从受控主机传回的变量等。使用 Ansible 的项目具有高可读性（人 & 计算机）。\n\n简单来说，它具有这些特点：\n\n- 简单 👴\n  - 基于 yaml，高可读性\n  - 不需要特定的开发技能\n  - 基于已有的社区轮子快速部署\n- 强大 👍\n  - 应用部署\n  - 配置管理\n  - 定制工作流\n  - 管理应用的生命周期\n- 无代理 👏\n  - 无代理架构\n  - 基于 OpenSSH & WRM(远程 Powershell)\n  - 无需更新配置\n  - 更安全可靠、更高效\n\n### Ansible 架构\n\n![img](https://latina-1253549750.cos.ap-shanghai.myqcloud.com/essay/imgs/20200702003725.png)\n\nAnsible 架构简单直接，Inventory 代表受管理的节点或其他资源，用户在建立控制节点到受控节点的连接后，通过直接指定命令或编写一种叫做 Playbook 的配置文件来执行特定的任务。对于 Linux 节点，使用 SSH 建立连接；而对于 Windows 节点，则使用 WRM(Windows Remote Management) 服务。\n\n### Ansible 对象\n\n在 Ansible 中，我们通常会用到这些对象：\n\n- Inventory：受控资源，如主机\n- Modules：预定义且可扩展的用于执行特定任务的模块\n- Tasks：用户描述的特定的任务\n- Playbooks：用户描述的特定任务集，描述一系列特定的任务及配套行为\n\n![image-20200703122053922](https://latina-1253549750.cos.ap-shanghai.myqcloud.com/essay/imgs/20200703122054.png)\n\n此外，Ansible 还有一个类似于 GitHub 的社区，叫 Ansible Galaxy。在这个社区中，有大量社区成员预定义的解决特定任务的工具包，我们既可以直接下载使用，也可以将合适的工具包集成到自有的 Playbooks 中。\n\n## 在实验室的应用案例\n\n### 需求背景\n\n实验室中有许多的老项目，这些项目的测试和生产环境尚未分离，测试依赖于 Monkey Test；部署和运行依赖于特定的运行环境，且需要大量的手工操作，成功率较低。\n\n迁移这些老项目到新的 Kubernetes 集群的开发成本较高，因此我们希望借助于 Ansible 来解决这些问题，实现这些功能：\n\n- 测试和生产环境分离\n- 旧项目依赖的版本控制\n- 权限控制\n- 不同依赖之间环境隔离\n- 可追踪、回溯、扩展\n- 自动部署\n\n### 具体应用\n\n对于旧项目，在坚持采用规范的开发流程的前提下，一个新的 Feature 从第一个 MR（Merge Request 合并请求） 到成功上线在时序图中需要超过 30 步的操作。这样的流程过度依赖人工，不同的环境依赖于不同的 GitLab Runner，而 Runner 的设置与具体项目耦合，频繁更改单个项目的 Runner 容易出错，而为测试和生产项目设置不同的 Runner 则会徒增许多重复的项目，衍生出许多其他的问题。\n\n![image-20200702004157733](https://latina-1253549750.cos.ap-shanghai.myqcloud.com/essay/imgs/20200702004157.png)\n\n因此，我们将 GitLab CI 和 Ansible 结合。具体来说包含这些关键步骤：\n\n- 准备多台主机（虚拟），并添加到 Ansible Inventory 中\n- 在 GitLab 创建一个用于管理旧项目基础架构的项目，并指定 Runner 为 Ansible 控制节点\n- 在 Ansible 项目中，为生产环境和测试环境指定不同的 Inventory，创建 Playbook 定义旧项目构建、部署、测试任务的具体行为、状态和依赖的环境\n- 定义 GitLab CI 流水线，配置持续集成和持续部署\n\n通过这些关键步骤，我们实现了预期的功能。具体来说：\n\n1. 项目开发者提交代码后触发 GitLab 流水线，Ansible 控制节点会运行 Playbook 中定义的行为，在测试环境中更新依赖并运行测试任务\n2. 在 MR 通过后 Ansible 控制节点自动在预发布(Staging)环境中部署\n3. 项目管理者确认 Staging 环境中功能正常后，点击 GitLab 流水线的手动操作即可按照设定比例 10%，50%，100% 等更新生产环境。\n\n下图是我们 Ansible 项目中的具体文件，其中 `.gitlab-ci.yml` 是 GitLab CI 流水线配置文件，而 `build.yaml`, `test.yaml` 和 `deploy.yaml` 则是 Ansible Playbook 文件。通过将项目的依赖和行为描述成 YAML 文件，我们可以将依赖、环境、行为的更新转化成对代码仓库的一次 MR。\n\n![image-20200703125905619](https://latina-1253549750.cos.ap-shanghai.myqcloud.com/essay/imgs/20200703125905.png)\n\n下图是流水线运行的截图，其中 `deploy` 是手动操作，用于将项目部署到 `staging` 环境。\n\n![image-20200703125803430](https://latina-1253549750.cos.ap-shanghai.myqcloud.com/essay/imgs/20200703125803.png)\n\n### 效果对比\n\n某不愿透露姓名的项目组成员表示，对于旧项目的更新和部署，再也不用担心秃头了。\n\n![image-20200703123501733](https://latina-1253549750.cos.ap-shanghai.myqcloud.com/essay/imgs/20200703123501.png)\n\n### Ansible 最佳实践指南\n\n根据我们的开发经验和 Ansible 官方指南，我们认为基于 Ansible 开发需要遵循以下原则：\n\n- 代码越复杂，生产效率越低\n- 提高 Playbook 中的任务和变量定义的可读性\n- 依据声明式的思想编写 Playbook 文件，Playbook 不是 Coding\n- 请为 Ansible Playbook 编写单元测试\n\n### Ansible 的局限\n\n我们也发现了 Ansible 的一些局限，主要是：\n\n1. Ansible 比较适合部署老的项目，新的项目不建议使用，请直接上 Kubernetes 谢谢。\n2. Ansible 不支持服务的底层环境隔离，如网络、系统内核版本等。\n3. Ansible 的响应速度不行，尤其是文件传输速度太慢（如果无法忍受，可以自己写 Module，根据特定业务需求扩展文件传输算法，使用文件分块算法等方式）\n\n对于新项目的更优的选择：以 Kubernetes, OpenStack, Openshift 为代表的服务编排平台，支持操作系统即代码、服务编排、网络隔离等高级特性。\n\n### 效果与展望\n\n实验室的大多数项目已经部署在了 Kubernetes 上，对于老项目而言，Ansible 无疑是最佳选择。通常，实验室中一个项目组使用了 Ansible 后，其他项目组也可以参考已有的写法快速实现实现自己项目的自动化部署。\n\n安全性、可维护性、可扩展性和易用性是我们的追求，相信随着 DevOps 技术的发展，未来我们可以为实验室项目成员提供更好的 DevOps 体验。\n\n## 参考资料\n\n> - Infrastructure as Code: https://en.wikipedia.org/wiki/Infrastructure_as_code\n> - Ansible Repository: https://github.com/ansible\n> - YAML language spec: https://yaml.org/spec/1.2/spec.html\n> - Ansible official documentation: https://docs.ansible.com/\n> - Ansible Galaxy: http://galaxy.ansible.com/\n> - Ansible Module Index: https://docs.ansible.com/ansible/latest/modules/modules_by_category.html\n> - GitLab CI official documentation: https://docs.gitlab.com/ce/ci/\n> - YAML Syntax: https://docs.ansible.com/ansible/latest/reference_appendices/YAMLSyntax.html\n> - Ansible Best Practices ANSIBLE BEST PRACTICES: THE ESSENTIALS\n"},{"title":"Secret & Config in K8s and Helm","url":"/articles/2020/06/29/kubernetes/secret-config-in-k8s/","content":"\n- Secret 在 helm 中可以很方便地通过 template 来定义，并会自动创建到对应命名空间\n- 可以将 secret 作为 volume 挂载到 pod\n- 挂载的地址不能是 / 根目录，否则会将容器的根目录都设置成虚拟化的目录\n- 可以根据需要，将 secret 中的对应的 key 映射到具体的文件名\n"},{"title":"Advance in Kubernetes (1)","url":"/articles/2020/06/20/kubernetes/focal-points-in-k8s/","content":"\n## 修复 RethinkDB 中尚存在的问题\n\n- persistentVolumeClaim 拿不到 PV 的原因未知，可能是 pv 没创建对？可能是 pvc 的要求不满足？\n- 尝试了 hostPath 但尚未尝试其他类型的 pv\n\n## 存储卷\n\n- Volume 和 PersistentVolume 用于 pod 存储数据\n- hostPath 作为 host 上的一个物理的文件夹不支持并发读写\n\n## Affinity 和 AntiAffinity\n\n- Pod 和 Node 都能够设置相互的亲和度和逆亲和度\n- 通过 Label 或者其他的一些可哈希的具有唯一标志的资源来判断\n\n## Taint 和 Tolerance\n\n- 可以设置 Node 存在一些「污点」，即 Taint ，只有忍受 Node 的**全部**污点的 Pod 才能够被分配到这个有污点的 Node 上乃至在上面运行\n- Node 偶尔会根据污点「驱逐」一些 Pod ，即使 Pod 忍受了这些污点，还是可能被驱逐。因此还可以设置 Pod 被驱逐后在 Node 上存活的时间长度\n\n## 几种重要的资源的理解\n\n- ReplicaController 是比较基础的资源，用于定义一些 Pod\n- Deployment 是早期定义一组服务的比较常用的资源，后来被 Statefule-Set 所取代\n- Service 不包含 Pod 定义，主要用在服务发现中，定义集群向外暴露的服务\n- Ingress 使用反向代理和服务发现机制，将给定 Name 的 Service 通过特定的 Port 和 HostName 暴露出去\n\n以上所有资源都可以通过 `kubectl create -f aaaa.yaml` 来创建\n\n## 使用 Helm\n\n使用 Helm 管理部署非常简单，但偶尔还需要手动改一下项目的 Chart 模板。\n"},{"title":"使用 Helm 部署有状态服务到 Kubernetes 上","url":"/articles/2020/06/18/kubernetes/deploy-stateful-services/","content":"\n## 原生 k8s 部署有状态服务\n\n### Related k8s resources\n\n`ConfigMap`: 公开的键值对，在运行时会将配置文件、命令行参数、环境变量、端口号以及其他配置软件工件绑定到 Pod 的容器和系统组件。将配置与 Pod 和组件分开，这有助于保持工作负载的可移植性，使其配置更易于更改和管理，并防止将配置数据硬编码到 Pod 规范\n\n`Secret`: 私有的、加密的，存储敏感配置信息\n\n`Service`:\n\n`PersistentVolumeClaim`: 被集群动态分配的持久化存储\n\n`Deployment`:**不具有唯一标识**的一组多个相同的 Pod，运行应用的多个副本，自动替换失败或者无响应的实例以确保所需的数量的 Pod。比较适合**无状态应用**，使用 Pod 模板。[GCP 文档](https://cloud.google.com/kubernetes-engine/docs/concepts/deployment?hl=zh-cn)\n\n`Pod`: 最基本的对象，包括一个或者多个容器\n\n`StatefulSet`: 与 Deployment 相对，表示具有唯一标识的一组 Pod。用于部署[有状态应用](https://cloud.google.com/kubernetes-engine/docs/how-to/deploying-workloads-overview?hl=zh-cn#stateful_applications)和集群应用，以将数据保存到永久性存储空间，适合部署 Kafka、MySQL、Redis、ZooKeeper 以及其他需要唯一持久身份和稳定主机名的应用。\n"},{"title":"Understanding Javascript Async","url":"/articles/2020/06/14/front-end/understanding-js-async/","content":"\n## Heap & Call Stack\n\nExecution contexts in call stack.\n\n![image-20191017184233049](/Users/mingshicai/Library/Application Support/typora-user-images/image-20191017184233049.png)\n\n## Single Thread & Callback\n\nDo one thing at a time. Callback added to a queue.\n\nCode to be call back later.\n\n## Callback Queue\n\nCallbacks in a queue, FIFO.\n\n## Microtask & Macrotask\n\nmicrotask in the background / task queue.\n\nmacrotask in the main thread.\n\n## Event Loop in Javascript\n\nWait until the **call stack** is clear, pop a **callback** and push it into **call stack**.\n\n## Microtask & Repaint\n\n![image-20191017191719769](/Users/mingshicai/Library/Application Support/typora-user-images/image-20191017191719769.png)\n\n![image-20191017192155393](/Users/mingshicai/Library/Application Support/typora-user-images/image-20191017192155393.png)\n\n## Webworkers\n\nIt has its own event loop.\n\n## Promise & Resolve\n\nA promise object, takes `resolve`, `reject` **callbacks**, with parameters as `value` or `reason`.\n\nWhen we call a promise, `promiseObject.then((value)=>{}).catch((reason)=>{})`.\n\nReturn a promise object in a function intead of **nested callbacks**.\n\n`Promise.all()` runs every single promise, and return all the `resolve` or `reject` parameters.\n\n```javascript\nPromise.all(p1, p2, p3)\n  .then((values) => {})\n  .catch((errors) => {});\n```\n\n`Promise.race()` it will return as the first promise resolved and return the single `resolve` value.\n\n## Async / Await\n\n[Reference on Youtube, The async await episode | promise](https://www.youtube.com/watch?v=vn3tm0quoqE)\n\nMake async code read like sync code.\n\nFollowing code automatically return a promise object.\n\n```javascript\nconst getEggs = async () => {\n  let eggs = [\"e\", \"g\", \"g\", \"s\"];\n  return eggs;\n};\n\nlet eggs = await getEggs();\n```\n\nOr make code simpler:\n\n```javascript\n// with async / await\nconst cookEggs = async () => {\n  let a = await getEggs();\n  let b = await getEggs();\n\n  return [a, b];\n};\n```\n\n`await` will pause the code, use `Promise.all` to accelerate your code when necessary:\n\n```javascript\nconst cookEggs = async () => {\n  let a = getEggs();\n  let b = getEggs();\n  result = Promise.all([a, b]);\n  return result;\n};\n```\n"},{"title":"Kubernetes 总结 (1)","url":"/articles/2020/06/14/kubernetes/conclusion-in-k8s/","content":"\n## 简单版本\n\n### 部署一个 docker 应用\n\n**下面都没指定 namespace，但实际很需要。**\n\n创建一个 deployment 资源。\n\n`kubectl run hello-web --image=IMAGE_URL --port PORT`\n\n暴露 pod 端口，创建一个 service 资源。\n\n`kubectl expose deployment hello-web --type=LoadBalancer --port 80 --target-port 8080`\n\n扩展应用。\n\n`kubectl scale deployment hello-web --replicas=3`\n\n### 更新应用\n\n直接设置 deployment 的镜像即可。\n\n`kubectl set image deployment/hello-web hello-web=IMAGE_URL`\n\n## 使用 Ingress 负载平衡\n\n### 创建 ingress 资源\n\n在简单版本中，创建完了 service 资源后，可以查看 service 的集群地址和端口。\n\n`kubectl get service web`\n\n创建 Ingress 资源，它封装了一系列规则和配置，可将外部 HTTP(S) 流量路由到内部服务。\n\n创建类似下面的文件：\n\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: basic-ingress\nspec:\n  backend:\n    serviceName: web\n    servicePort: 8080\n```\n\n然后执行 `kubectl apply -f basic-ingress.yaml`，过一会儿\n\n执行 `kubectl get ingress basic-ingress`，查看负载平衡器的外部 IP 地址，如：\n\n```shell\nNAME            HOSTS     ADDRESS         PORTS     AGE\nbasic-ingress   *         203.0.113.12    80        2m\n```\n\n就能通过外部地址访问了。\n\n### 配置 静态 ip、不同子路径\n\n在创建 ingress 后，指定了端口，并获得了 ip，但存在两个问题：\n\n1. 外部 ip 不是静态的，会发生变化\n1. 端口 8080 肯定会与其他应用冲突，我们需要使用反向代理把需要的 web 服务 8080 端口暴露出去，并通过 hostName 区分不同的服务\n\n因此我们需要静态 ip、dns 服务和反向代理服务。\n\n更新 ingress yaml 文件：\n\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: basic-ingress\n  annotations:\n    kubernetes.io/ingress.global-static-ip-name: \"web-static-ip\"\nspec:\n  backend:\n    serviceName: web\n    servicePort: 8080\n```\n\n也可以在一个 ingress 服务中处理多个应用，参考上面的步骤创建另一个 web 服务，然后更新 ingress 文件：\n\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: fanout-ingress\nspec:\n  rules:\n    - http:\n        paths:\n          - path: /*\n            backend:\n              serviceName: web\n              servicePort: 8080\n          - path: /v2/*\n            backend:\n              serviceName: web2\n              servicePort: 8080\n```\n\n注意：**这样能够实现同一个域名使用不同子路径提供不同的 web 服务，比如 api 服务和 web 服务**\n\n### 反向代理与 k8s 上的 DNS 服务\n"},{"title":"Focal Points in React.js","url":"/articles/2020/06/12/front-end/react-focal-points/","content":"\n## 基础回顾\n\n### 生命周期\n\n- 挂载\n  - componentDidMount\n  - componentWillMount\n  - constructor\n  - rendoer\n- 更新\n  - render\n  - shouldComponentUpdate\n  - componentDidUpdate\n- 卸载\n  - componentWillUnmount\n\n### 事件处理\n\n这样定义事件处理需要进行绑定：\n\n```javascript\nclass A extends Component {\n  constructor(props) {\n    super(props);\n    this.handleBClick = this.handleBClick.bind(this);\n  }\n  handleBClick() {\n    //\n  }\n}\n```\n\n这样就不需要：\n\n```javascript\nclass A extends Component {\n  constructor(props) {\n    //\n    super(props);\n  }\n  handleBClick = () => {\n    //\n  };\n}\n```\n\n### Props\n\nReact 中，props 作为函数的参数，可以使用任何类型的对象，包括 Component, Function, js 对象等。\n\n### 数据分发与数据提升\n\n单向数据流，父组件通过 props 向子组件分发数据，子组件是**纯函数**。\n\n若子组件需要向父组件发送数据，使用父组件提供的 **props 中的函数**向父组件传递数据。\n\n父组件可以通过 Context 向子组件广播数据，而避免通过 props 层层传递，一个简单的 demo：\n\n```javascript\n// 创建 Context\nconst ThemeContext = React.createContext(\"lightOption\");\n\nclass A extends Component {\n  render() {\n    return (\n      <ThemeContext.Provider value=\"dark\">\n        <Toolbar />\n      </ThemeContext.Provider>\n    );\n  }\n}\n\nfunction Toolbar(props) {\n  return (\n    <div>\n      <ThemedButton />\n    </div>\n  );\n}\n\nclass ThemedButton extends Component {\n  // 需要指定 contextType 静态\n  static contextType = ThemeContext;\n  render() {\n    // 向上找最近的 Provider 并使用其值\n    return <button theme={this.props.context} />;\n  }\n}\n```\n\nprops 中可以传递 Component，因此也可以在顶部直接把数据包装到 Component 中然后传递给子组件，这样会讲逻辑提升到组件树的更高层次、使得高层组件更复杂。\n除此之外，还可以使用 render props 来共享数据和代码逻辑。\n"},{"title":"MySQL 主从复制","url":"/articles/2020/05/30/database/mysql-master-slave/","content":"\n## 流程\n\n更新 master 上的 /etc/mysql/m.cnf 文件\n\n```\n[mysqld]\nlog-bin=mysql-bin #开启二进制日志\nserver-id=1 #设置server-id\n```\n\n```sql\ncreate user 'repl'@'%' identified with mysql_native_password by 'password';\n\ngrant replication slave on *.* to 'repl'@'%';\n\nflush privileges;\n\nchange master to master_host='192.168.0.82',master_port=32795, master_user='repl', master_password='password', master_log_file='mysql-bin.000001', master_log_pos=1413, master_connect_retry=30;\n\nchange master to master_host='172.17.0.3', master_user='repl', master_password='password', master_log_file='mysql-bin.000001', master_log_pos=825, master_connect_retry=30;\n\n-- message: Authentication plugin 'caching_sha2_password' reported error: Authentication requires secure connection\n-- change the encryption of password\nALTER USER 'yourusername'@'localhost' IDENTIFIED WITH mysql_native_password BY 'youpassword';\n```\n\n可以指定同步的数据库和表以及日志的保存时间。\n\n## 要求\n\n无论主从还是双主，slave 需要通过固定 ip 访问到 master 服务器。因此，需要设置本地 master 的 bin log 的持续时间，并在该时间内让远程的 slave 连接到本地的 master 进行同步。\n\n腾讯云的 saas 的 mysql 服务没法配置主从备份。\n\n## 参考资料\n\n[博客园文章](https://www.cnblogs.com/gl-developer/p/6170423.html)\n"},{"title":"混合云中的 MySQL 备份实战","url":"/articles/2020/05/26/database/mysql-backup-in-action/","content":"\n## 步骤\n\n### 备份\n\n1. 在路由器上连接到 VPN\n2. 在腾讯云 VPN 服务器上设置路由表，使其可以访问到 LAN 上的 Mysql 服务器\n3. 在腾讯云 VPN 服务器上设置 Nginx 反向代理 LAN 的 Mysql 服务器\n4. （可选）在本地创建用于备份的 mysql 账户，需要`replication`, `slave`权限\n5. 本地测试通过腾讯云 VPN 服务器的公网 ip + nginx 设置的端口号连接本地 mysql 服务器\n6. 若连接不成功，请检查 nginx 配置和 vpn 连接是否正常\n7. 在阿里云上购买`数据库备份DBS`\n8. 设置备份计划，设置备份源为腾讯云上的 VPN 服务器的公网 ip + nginx 上设置的端口号\n9. 启动备份计划，包括全量备份和增量备份\n\n第 3 步中 Nginx 配置文件 `/etc/nginx/nginx.conf`，一般是这个。注意通过 `stream` 而不是 `http`，具体文件内容：\n\n```\nhttp {\n    ...\n}\n\nstream {\n    upstream mysql_lan_106 {\n        server 192.168.0.106:3306;\n    }\n\n    server {\n        listen 33060;\n        proxy_pass mysql_lan_106;\n    }\n}\n\n```\n\n更新后，运行 `sudo nginx -s reload` 来重启 nginx。\n\n### 恢复\n\n在阿里云 dbs 中选择`恢复任务`，设置恢复全量+增量的备份，设置目的数据库为已经可以通过腾讯云主机上搭建的 VPN 访问的本地 Mysql 服务器，配置已经有合适权限的账号。在启动恢复任务前会检查，如果出错，按照提示配置即可。\n\n### 注意\n\n1. 阿里云的备份计划只能同时备份一个源数据库；\n2. 保持 VPN 连接。阿里云会在一定时间内尝试重新连接源/目的数据库，超时后任务失败，无法断点续传；\n3. 可以通过有公网 ip 的 VPN 服务器反向代理本地的任意 Mysql 服务。\n\n## 实际备份测试\n\n购买的是一个月的 small 版的逻辑备份计划，花费 140 元，包含 400GB/月 的流量，超出的流量 0.35 元/GB，单独购买存储包无法进行备份，但可以抵扣超出的流量（貌似）。\n\n备份：测试备份和恢复 106 数据库上的 finance 库，传输速度约为 1MB/s 左右，备份耗时大约 1 小时，备份大小 23.63GB。\n恢复：\n\n## 不可行的方案\n\n### 使用腾讯云 mysql 实例进行主从或双主备份\n\n- 腾讯云 mysql 实例的账号没有权限用来配置 master/slave\n\n## 参考资料\n"},{"title":"MySQL 锁相关知识总结","url":"/articles/2020/05/22/database/locks-in-mysql/","content":"\n## 事务\n\n数据库事务: **事务(Transaction)**，一般是指要做的或所做的事情。在计算机术语中是指访问并可能更新数据库中各种数据项的一个程序执行单元(unit)。在计算机术语中，事务通常就是指数据库事务。\n\n一个数据库事务通常包含对数据库进行读或写的一个操作序列。它的存在包含有以下两个目的：\n\n> 1、为数据库操作提供了一个从失败中恢复到正常状态的方法，同时提供了数据库即使在异常状态下仍能保持一致性的方法。\n> 2、当多个应用程序在并发访问数据库时，可以在这些应用程序之间提供一个隔离方法，以防止彼此的操作互相干扰。\n\n即:1.当一个事务被提交给了 DBMS（数据库管理系统），则 DBMS 需要确保该事务中的所有操作都成功完成且其结果被永久保存在数据库中，如果事务中有的操作没有成功完成，则事务中的所有操作都需要被回滚，回到事务执行前的状态（要么全执行，要么全都不执行）; 2.同时，该事务对数据库或者其他事务的执行无影响，所有的事务都好像在独立的运行。\n\n**事务的 4 个属性 acid**: atom 原子性,作为一个整体被执行,consistency 一致性,数据满足完整性约束,isolation 隔离性,不影响其他事物,durability 持久性,事务执行成功后对数据库的修改被永久记录.\n\n## 事务隔离级别\n\nMysql 可以设置 4 个隔离级别,允许用户在性能和可靠性之间作出选择.\n\nRead uncommitted, **会出现读到了未提交的数据(脏读);** 事务 b 读到了事务 a 尚未提交的数据,该数据可能会被事务 a 回滚.即**脏读**.\n\nread committed, **会出现读到的数据被别的事务提交更新了,也就是在同一个事务中,两次读到的数据不是一致的**(**不可重复读**); 事务 a 先读取了数据,事务 b 也读区了,但 b 更新了数据并提交了事务,而 a 再次读数据时,数据已经变了.即**不可重复读**.\n\nrepeatable read, mysql 的默认隔离级别;尽管通过 **Next-Key Lock** 某种程度上解决了幻读读问题,但仍可能出现幻读(事务在插入事先检测不存在的记录时，惊奇的发现这些数据已经存在了，之前的检测读获取到的数据如同鬼影一般。**\\*在一个事务中，同一个范围内的记录被读取时，其他事务向这个范围添加了新的记录**。\\*).\n\n**幻读例子**: 重新开启了两个会话 `SESSION 1` 和 `SESSION 2`，在 `SESSION 1` 中我们查询全表的信息，没有得到任何记录；在 `SESSION 2` 中向表中插入一条数据并提交；由于 `REPEATABLE READ` 的原因，再次查询全表的数据时，我们获得到的仍然是空集，但是在向表中插入同样的数据却出现了错误。\n\n这种现象在数据库中就被称作幻读，**虽然我们使用查询语句得到了一个空的集合，但是插入数据时却得到了错误，好像之前的查询是幻觉一样**。\n\nserializable.**串行**;最高级别,但是代价高,性能低,一般很少使用,**事务串行执行**.通常,在实际应用中自己加锁来避免幻读.\n\n|     | 脏读     | 不可重复读 | 幻读     |\n| --- | -------- | ---------- | -------- |\n| Ru  | 可能出现 | 可能出现   | 可能出现 |\n| Rc  |          | 可能出现   | 可能出现 |\n| Rr  |          |            | 可能出现 |\n| S   |          |            |          |\n\n## 并发控制\n\n**悲观锁**：假定会发生并发冲突，\\*\\*屏蔽一切可能违反数据完整性的操作。InnoDB 使用的是悲观锁.\n\n**乐观锁**：假设不会发生并发冲突，**只在提交操作时检查是否违反数据完整性**。不是一种真正的锁,冲突了就重试,没有对数据库加锁.**乐观锁因为没有真正加锁,所以不能解决脏读的问题**。它会先尝试对资源进行修改，在写回时判断资源是否进行了改变，如果没有发生改变就会写回，否则就会进行重试，\n\n乐观锁不会存在死锁的问题，但是由于更新后验证，所以当**冲突频率**和**重试成本**较高时更推荐使用悲观锁，而需要非常高的**响应速度**并且**并发量**非常大的时候使用乐观锁就能较好的解决问题，在这时使用悲观锁就可能出现严重的性能问题；在选择并发控制机制时，需要综合考虑上面的四个方面（冲突频率、重试成本、响应速度和并发量）进行选择。\n\n## 锁的分类\n\n**共享锁**代表了读操作、互斥锁代表了写操作，所以我们可以在数据库中**并行读**，但是只能**串行写**，只有这样才能保证不会发生线程竞争，实现线程安全。\n\n锁的粒度,包括**行锁、表锁**,还引入了**意向锁**,是一种表级锁.\n\n**意向锁**其实不会阻塞全表扫描之外的任何请求，它们的主要目的是为了表示**是否有人请求锁定表中的某一行数据**。\n\n理解意向锁的目的: 我们在这里可以举一个例子：如果没有意向锁，当已经有人使用行锁对表中的某一行进行修改时，如果另外一个请求要对全表进行修改，那么就需要对所有的行是否被锁定进行扫描，在这种情况下，效率是非常低的；不过，在引入意向锁之后，当有人使用行锁对表中的某一行进行修改之前，会先为表添加意向互斥锁（IX），再为行记录添加互斥锁（X），**在这时如果有人尝试对全表进行修改就不需要判断表中的每一行数据是否被加锁了，只需要通过等待意向互斥锁被释放就可以了。**\n\n## 锁的算法\n\n**记录锁 record lock** 添加到**索引记录**上的锁.在建表时如果指定了 `key` 那么,InnoDB 就能通过 B+树找到行记录并添加索引.否则,不知道待修改的记录具体的位置,只能锁定整个表.\n\n**间隙锁 gap lock** 是对索引记录中的一段连续区域的锁；当使用类似 `SELECT * FROM users WHERE id BETWEEN 10 AND 20 FOR UPDATE;` 的 SQL 语句时，就会阻止其他事务向表中插入 `id = 15` 的记录，因为**整个范围都被间隙锁锁定**了。\n\n> _间隙锁是存储引擎对于性能和并发做出的权衡，并且只用于某些事务隔离级别。_\n\n虽然间隙锁中也分为共享锁和互斥锁，不过它们之间并不是互斥的，也就是不同的事务可以同时持有一段相同范围的共享锁和互斥锁，它唯一阻止的就是**其他事务向这个范围中添加新的记录**。\n\n**记录锁和间隙锁的结合:Next-Key Lock** 既然叫 Next-Key 锁，锁定的应该是当前值和后面的范围，但是实际上却不是，Next-Key 锁锁定的是当前值和前面的范围.当我们更新一条记录，比如 `SELECT * FROM users WHERE age = 30 FOR UPDATE;`，InnoDB 不仅会在范围 `(21, 30]` 上加 Next-Key 锁，还会在这条记录后面的范围 `(30, 40]` 加间隙锁，所以插入 `(21, 40]` 范围内的记录都会被锁定。\n\nNext-Key 锁的作用其实是为了解决**幻读**的问题.\n\n**不使用索引来更新,会导致表锁的情况.**\n\n参考资料:[『浅入浅出』MySQL 和 InnoDB](https://draveness.me/mysql-innodb#)\n"},{"title":"How to Debug Those Projects","url":"/articles/2020/04/25/guide/how-to-debug/","content":"\nThis article aims to clarify how-to in debugging for following popular projects:\n\n- Kubernetes related\n  - Helm\n  - Ingress\n  - Service/Deployment/Pod\n\n\n\n:TODO\n\n\n\n\n\n"},{"title":"Golang Concurrency Pattern","url":"/articles/2020/04/14/golang/go-concurrency/","content":"\n注意：以下所说的线程、进程、协程都是指一个东西：`goroutine`\n\n## 5 种模式\n\n- 最大输入的管道\n- 任务分解多个协程，结果彼此无关\n- 线程安全的数据结构，不需要锁\n- 三种方法，将任务分成固定多个协程，并在最后合并结果\n- 创建进程依赖的数量的协程，并在最后合并结果\n\n## 死锁\n\n- 因为没有完成信号，所有协程都没有退出，即使任务已经完成\n- 阻塞的线程想要获得其它正在阻塞的线程的锁\n\n使用 done 通道报告结果。\n\n使用 sync.WaitGroup ，但当其它线程正在阻塞，而 sync.WaitGroup.Wait() 在主进程被调用时，就会出现死锁。\n\n使用 Channel 依然会导致死锁，比如\n\n## 线程安全性\n\n安全：bool, int, float64, string\n\n不安全：\n\n- 引用类型，除非 1.互斥量 2.访问规则 3.不修改\n- 定义了修改的接口类型\n\n没有缓冲区的通道会立即阻塞，直到有其它进程从中读取。缓冲区能够提高吞吐量。\n\n发送端的进程关闭通道\n\n单向通道表达程序思想\n\n通道关闭\n\n合理设置缓冲区大小和进程数量，尽可能将不必要的阻塞降低到最低\n\n带有通道参数的函数，通常将目标通道放在前面，源通道放在后面。\n\n双向通道才允许关闭\n\n使用非阻塞 select，可通过 timeout 退出\n\n线程安全的映射，能够被多个线程共享。只要按照 safemap/safemap.go 定义的 SafeMap 接口实现就行。\n\n> 可导出/不可导出的接口\n\n## 同步\n\n- 对于从无缓冲 Channel 进行的接收，发生在对该 Channel 进行的发送完成之前\n- 对于带缓冲的 Channel，对于 Channel 的第 K 个接收完成操作发生在第 K+C 个发送操作完成之前，其中 C 是 Channel 的缓存大小\n"},{"title":"Packages in Java","url":"/articles/2020/03/27/java/packages-in-java/","content":"\n## User Defined Packages\n\nFirst we create a directory myPackage (name should be same as the name of the package). Then create the MyClass inside the directory with the first statement being the package names.\n\n## Directory Structure\n\n`BASE_DIR` and `CLASSPATH`\n\nThe base directory ($BASE_DIR) could be located anywhere in the file system. Hence, the Java compiler and runtime must be informed about the location of the $BASE_DIR so as to locate the classes. This is accomplished by an environment variable called CLASSPATH. CLASSPATH is similar to another environment variable PATH, which is used by the command shell to search for the executable programs.\n\n可以将 jar 包直接加到 `CLASSPATH` 中，然后就可以正常引用这个包了。\n\n## Ref\n\n[geeksforgeeks](https://www.geeksforgeeks.org/packages-in-java/)\n"},{"title":"Null Things in Java","url":"/articles/2020/03/22/java/null-thins-in-java/","content":"\n## Boxing and Unboxing null object\n\n```java\npublic class Test\n{\n\tpublic static void main (String[] args) throws java.lang.Exception\n\t{\n\t\t\t//An integer can be null, so this is fine\n\t\t\tInteger i = null;\n\n\t\t\t//Unboxing null to integer throws NullpointerException\n\t\t\tint a = i;\n\t}\n}\n```\n\n## Instanceof\n\n```java\npublic class Test\n{\n\tpublic static void main (String[] args) throws java.lang.Exception\n\t{\n\t\tInteger i = null;\n\t\tInteger j = 10;\n\n\t\t//prints false\n\t\tSystem.out.println(i instanceof Integer);\n\n\t\t//Compiles successfully\n\t\tSystem.out.println(j instanceof Integer);\n\t}\n}\n```\n\n## Static and Non-Static\n\n```java\npublic class Test\n{\n\tpublic static void main(String args[])\n\t{\n\t\tTest obj= null;\n\t\tobj.staticMethod();\n\t\tobj.nonStaticMethod();\n\t}\n\n\tprivate static void staticMethod()\n\t{\n\t\t//Can be called by null reference\n\t\tSystem.out.println(\"static method, can be called by null reference\");\n\t}\n\n\tprivate void nonStaticMethod()\n\t{\n\t\t//Can not be called by null reference\n\t\tSystem.out.print(\" Non-static method- \");\n\t\tSystem.out.println(\"cannot be called by null reference\");\n\t}\n}\n```\n\n## Comparison\n\n`null == null`, `null != null`\n\n`true, false`\n"},{"title":"Currying Functions with `java.util.function.Function`","url":"/articles/2020/03/16/java/currying-funcs-in-java/","content":"\n```java\n// Java Program to demonstrate Function Currying\n\nimport java.util.function.Function;\n\npublic class GFG {\n\tpublic static void main(String args[])\n\t{\n\n\t\t// Using Java 8 Functions\n\t\t// to create lambda expressions for functions\n\t\t// and with this, applying Function Currying\n\n\t\t// Curried Function for Multiplying u & v\n\t\tFunction<Integer,\n\t\t\t\tFunction<Integer, Integer> >\n\t\t\tcurryMulti = u -> v -> u * v;\n\n\t\t// Calling the curried functions\n\n\t\t// Calling Curried Function for Multiplying u & v\n\t\tSystem.out.println(\"Multiply 2, 3 :\"\n\t\t\t\t\t\t+ curryMulti\n\t\t\t\t\t\t\t\t.apply(2)\n\t\t\t\t\t\t\t\t.apply(3));\n\t}\n}\n```\n"},{"title":"Constructor Chaining","url":"/articles/2020/03/12/java/constructer-chaining/","content":"\nFollowing code demonstrates it:\n\n```java\nclass Test\n{\n    final public int i;\n\n    Test(int val)    {  this.i = val;  }\n\n    Test()\n    {\n        // Calling Test(int val)\n        this(10);\n    }\n\n    public static void main(String[] args)\n    {\n        Test t1 = new Test();\n        System.out.println(t1.i);\n\n        Test t2 = new Test(20);\n        System.out.println(t2.i);\n    }\n}\n```\n"},{"title":"Bounded Type Parameter","url":"/articles/2020/03/02/java/bounded-type-parameter-in-java/","content":"\n## Bounded Type\n\n```java\n// This class only accepts type parametes as any class\n// which extends class A or class A itself.\n// Passing any other type will cause compiler time error\n\n// LOOK AT ME!\nclass Bound<T extends A>\n{\n\n\tprivate T objRef;\n\n\tpublic Bound(T obj){\n\t\tthis.objRef = obj;\n\t}\n\n\tpublic void doRunTest(){\n\t\tthis.objRef.displayClass();\n\t}\n}\n\nclass A\n{\n\tpublic void displayClass()\n\t{\n\t\tSystem.out.println(\"Inside super class A\");\n\t}\n}\n\nclass B extends A\n{\n\tpublic void displayClass()\n\t{\n\t\tSystem.out.println(\"Inside sub class B\");\n\t}\n}\n\nclass C extends A\n{\n\tpublic void displayClass()\n\t{\n\t\tSystem.out.println(\"Inside sub class C\");\n\t}\n}\n\npublic class BoundedClass\n{\n\tpublic static void main(String a[])\n\t{\n\n\t\t// Creating object of sub class C and\n\t\t// passing it to Bound as a type parameter.\n\t\tBound<C> bec = new Bound<C>(new C());\n\t\tbec.doRunTest();\n\n\t\t// Creating object of sub class B and\n\t\t// passing it to Bound as a type parameter.\n\t\tBound<B> beb = new Bound<B>(new B());\n\t\tbeb.doRunTest();\n\n\t\t// similarly passing super class A\n\t\tBound<A> bea = new Bound<A>(new A());\n\t\tbea.doRunTest();\n\n\t}\n}\n```\n\n## Multiple Bound\n\n```java\n// LOOK AT ME!\nclass Bound<T extends A & B>\n{\n\n\tprivate T objRef;\n\n\tpublic Bound(T obj){\n\t\tthis.objRef = obj;\n\t}\n\n\tpublic void doRunTest(){\n\t\tthis.objRef.displayClass();\n\t}\n}\n\ninterface B\n{\n\tpublic void displayClass();\n}\n\nclass A implements B\n{\n\tpublic void displayClass()\n\t{\n\t\tSystem.out.println(\"Inside super class A\");\n\t}\n}\n\npublic class BoundedClass\n{\n\tpublic static void main(String a[])\n\t{\n\t\t//Creating object of sub class A and\n\t\t//passing it to Bound as a type parameter.\n\t\tBound<A> bea = new Bound<A>(new A());\n\t\tbea.doRunTest();\n\n\t}\n}\n```\n\n## Ref\n\n[bounded type parameter](https://www.geeksforgeeks.org/bounded-types-generics-java/)\n"},{"title":"Best Practice of Dokuwiki Workflow, Better Than OneNote","url":"/articles/2019/10/18/tutorial/dokuwiki-workflow/","content":"\n## Motivation\n\n[DokuWiki](https://www.dokuwiki.org/dokuwiki) is a powerful self-host wiki system. Content in a DokuWiki site is all static s.t. we can easily backup or restore our documents.\n\nI use DokuWiki a lot, and I prefer to write markdown files locally on my computer and then create a page on DokuWiki web page.\nHowever, this is not suitable for well-organized files and takes a lot of time, besides, I alwayse forget to upload the files or just can't remember which files are not on the cloud.\n\nSo I need an automation workflow to help me simplify jobs above.\n\nGit is the perfect tool for this situation, it's distributed and easy to control version of files. So I created a DokuWiki workflow based on Git.\n\n## Structure\n\n![Structure](https://latina-1253549750.cos.ap-shanghai.myqcloud.com/essay/imgs/20191018191649.png)\n\n## Setup a DokuWiki Site\n\nJust create a docker container and mount a `volume` where you put the persistent DokuWiki files.\n\n## Backup\n\nGenerally, after I established a DokuWiki site, I just initialize a repository in DokuWiki's root directory, known as `dokuwiki/` and regularly push it to GitHub and other git mirrors as a backup using `crontab`.\n\n## Write locally & Update Remotely\n\nThe core content of a DokuWiki is inside the directory of `dokuwiki/data/pages`, so just initialize another sub repository here and push it to GitHub. Then pull the git repository to your local computer, push to GitHub repository after you update the wiki.\n\nThe DokuWiki site needs to keep pace with local repository, so add another `crontab` job to pull the git repository.\n\nNotice that files in DokuWiki directory is owned by `daemon` by default, after `sudo git pull` the files are owned by `root`, that is, DokuWiki itself cannot modify them, in other words, you cannot edit the pages on DokuWiki webpage, which is just what we want it to be in order to prevent version conflict.\n\n### Suggested Tool Chain\n\nDokuWiki uses `.txt` files as pages, in order to treat them as markdown files, you need to put a `workplace` setting file `.vscode/settings.json` for vscode. The file content maybe following:\n\n```json\n{\n  \"files.associations\": {\n    \"*.txt\": \"markdown\"\n  },\n  \"workbench.colorTheme\": \"Dracula\",\n  \"[markdown]\": {\n    \"editor.quickSuggestions\": true,\n    \"editor.wordWrap\": \"on\"\n  }\n}\n```\n\nFor further info about vscode settings and code-snippets please read the official documentation of vscode.\n\nThanks for reading!\n"},{"title":"Integrate InfluxDB Service in GitLab CI","url":"/articles/2019/08/22/cicd/integrate-influxdb-service-in-gitlab-ci/","content":"\nThe services keyword defines a Docker image that runs during a job linked to the Docker image that the image keyword defines. This allows you to access the service image during build time.\n\nThe service image can run any application, but the most common use case is to run a database container, for example:\n\n- MySQL/PostgreSQL/SQLServer\n- Redis\n\nGitLab didn't provide an official document for InfluxDB, it still needs to configure though a `service` is just an image.\n\nAccording to [InfluxDB's official docker image](https://hub.docker.com/_/influxdb), we need to set a few envrionment variables, such as:\n\n- INFLUXDB_DB: for initializing db\n- INFLUXDB_ADMIN_USER: for write/read data, relies on the var above\n\nI add a service to a job, following `.gitlab-ci.yml` describes it:\n\n```yml\n# Execute via shell\nservices:\n  - name: influxdb:latest\nvariables:\n  INFLUXDB_DB: \"transaction_test\"\n  INFLUX_DB_HOST: \"http://localhost:8086\"\n# ...\n```\n\nHowever, `Connection Refused` errors occurred when I tried to dial TCP to `localhost` on port `8086` through shell executor in GitLab CI. Port `8086` is influxdb docker image's default HTTP API port.\nWhen defining a `mysql` or `redis` service, no port was described in yaml file, and I successfully connected to `mysql` service via\n`localhost:3306`. Is there any difference between these images?\n\nThe `mysql` image bind port `3306` by default, yet `influxdb` has not. Besides, GitLab does not support port mapping in CI `service`. There is an [issue](https://gitlab.com/gitlab-org/gitlab-runner/issues/2460) about port mapping in CI `service`, but it's still WIP after 2 years.\n\nTo debug, we can run our job on local gitlab-runner, and add `tail -F /dev/null` line before the part where our script fails, this will hault the job for 30 minutes. While it hangs, we can attach to the container wit `docker exec -it container-name /bin/sh` to see what happens. But I'd not try this.\n\nFinally, I found that the executor was randomly `Docker executor` and `Kubernetes executor`. The address is different in different executor.\nIn `Docker executor`, its own dns service will resolve the service name to a typical ip address. However in `K8S executor` you must link to the service via exact ip address, such as `localhost` or `127.0.0.1`.\n\nThen I just add a tag to the job, it works. GitLab CI will automatically publish the `EXPOSEd` port in the image's `Dockerfile`.\n\n```yml\ntags:\n  - kubernetes\n```\n"},{"title":"Hybrid Cloud Across IPsec VPN Tunnel","url":"/articles/2019/05/30/tutorial/hybrid-cloud-cross-ipsec-vpn/","content":"\n## Tutorial\n\n- Install IPsec VPN on public cloud machine.\n- Connect the router in LAN to the VPN.\n- Configurate gateway in the cloud machine.\n- Setup NGINX on the cloud machine.\n- Add server conf file to `/etc/nginx/sites-enabled/` and write it in your way.\n- Reload your NGINX server and visit the URL to see what happens.\n\n## NGINX Conf Demo\n\n```\nserver {\n        listen 80;\n        server_name api.contoso.com;\n        location / {\n                proxy_pass http://192.168.0.70:8848;\n                proxy_set_header Host $http_host;\n        }\n}\n```\n\n## Tips\n\n- Ubuntu saves your life not centOS.\n"},{"title":"Simple Reason Why Need to Avoid Side Effect","url":"/articles/2019/05/03/guide/simple-reason-why-need-to-avoid-side-effect/","content":"\n## Counterexample\n\nMoments ago I wrote a mutation function in a Vue.js project with side-effect, which means I'd mutated the params.\n\n```typescript\n// the mutation function at a glance\n[types.MUTATION_NAME](state: State, data: DataInterface) {\n  data.foo = newObject\n  data.bar = newObject\n  ...\n}\n```\n\nBesides, I `watch` the params passed to the mutation function in the component. Then a infinite loop occurred. ;(\n\nThat's why we need to avoid side effect, a simple reason.\n\n"},{"title":"Solve Pickle Issue When Using Function with Decorator in Multiprocessing","url":"/articles/2019/04/26/python/solve-multiprocessing-decorator-pickle-issue/","content":"\n## Issue\n\nExceptions raised below when I tried to use function with function decorator in multiprocessing.\n\n### My Decorator\n\n```python\ndef wrapped(retry=5, with_client=False):\n    def decorator(func):\n        @wraps\n        def wrapper(*args, **kwargs):\n            nonlocal retry\n            while True:\n                try:\n                    if with_client:\n                        client = InfluxDBClient(**kwargs['influx_option'])\n                        func(client, *args, **kwargs)\n                    else:\n                        func(*args, **kwargs)\n                except Queue.empty:\n                    time.sleep(1)\n                    if retry < 0:\n                        print('Exit {} after retry.'.format(func.__name__))\n                    retry -= 1\n                except Queue.full:\n                    time.sleep(1)\n                except Exception as e:\n                    print('Exception raised in {}'.format(func.__name__))\n                    print(str(e))\n                    time.sleep(1)\n        return wrapper\n    return decorator\n```\n\nI use this decorator function this way:\n\n```python\nfrom multiprocessing import Process\n# ...\n\n@wrapped(4)\nrun(arg):\n    pass\n\nProcess(target=run, args=('test',)).start()\n```\n\n### Errors\n\n```bash\n# 1st error\nAttributeError: Can't pickle local object 'wrapped.<locals>.decorator.<locals>.wrapper'\n\n# 2nd error\nEOFError: Ran out of input\n```\n\n## Solution\n\nThe reason this happens is that a wrapped function with function decorator is not `picklable`, which means the wrapped function is not serializable when passed to `Process()`, view [this blog](http://ralph-wang.github.io/blog/2015/02/15/zhuang-shi-qi-yu-duo-jin-cheng-yi-ji-pickle/)(language: zh-CN).\n\nYou can implement a decorator class to return a `picklable` object instead of decorator function.\n\n### My New Decorator\n\n```python\nclass Wrapped(object):\n    def __init__(self, retry=5, with_client=False):\n        self.retry = retry\n        self.with_client = with_client\n\n    def __call__(self, func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            while True:\n                try:\n                    if self.with_client:\n                        client = InfluxDBClient(**kwargs['influx_option'])\n                        func(client=client, *args, **kwargs)\n                    else:\n                        func(*args, **kwargs)\n                except Queue.empty:\n                    time.sleep(1)\n                    if retry < 0:\n                        print('Exit {} after retry.'.format(\n                            func.__name__\n                        ))\n                        break\n                    retry -= 1\n                except Queue.full:\n                    time.sleep(1)\n                except Exception as e:\n                    print('Exception raised in {}'.format(\n                        func.__name__\n                    ))\n                    print(str(e))\n                    time.sleep(1)\n        return wrapper\n```\n\nNow, we use the decorator class this way:\n\n```python\n\n@Wrapped(4, True)\nrun(arg):\n    pass\n\nProcess(target=run, args=(arg, ))\n```\n\n"},{"title":"Install Python on Windows the Easy Way","url":"/articles/2019/04/26/tutorial/install-python-the-easy-way/","content":"\n## Background\n\nThe sane way to manage software on Windows is using `Chocolatey`. It makes it possible and uncomplicated to install softwares automatically by just one command as easy as abc.\n\nBesides, it's open source and totally free for personal use. There are over 6500 community maintained packages on `Chocolatey` includes common softwares like Python, Adobe PDF Reader, Web Browsers and Anaconda.\n\nThe install process is smooth, just type a line of command and click `Enter`, you will have your well configured software installed after a cup of tea.\n\n## Steps\n\n### Install Chocolatey\n\n> Checkout this [official tutorial](https://chocolatey.org/install) for installing `Chocolatey`.\n\n1. Right click the Windows icon on bottom-left of your screen, and click `Powershell (as Admin)` option like below:\n\n![t1](https://seccdn.unoiou.com/img/articles/tutorial-install-chocolatey/t1.png)\n\n1. When the User-Account-Control windows displays, click `Yes`.\n\n![t2](https://seccdn.unoiou.com/img/articles/tutorial-install-chocolatey/t2.png)\n\n1. Paste the scripts below to the window:\n\n```Powershell\nSet-ExecutionPolicy Bypass -Scope Process -Force; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\n```\n\n![t3](https://seccdn.unoiou.com/img/articles/tutorial-install-chocolatey/t3.png)\n\n1. Type `Enter`, when it finished, the window should be like below:\n\n![t4](https://seccdn.unoiou.com/img/articles/tutorial-install-chocolatey/t4.png)\n\n1. Check the installation. Type `choco` or `choco -?`, if you get result like below, it means the installation is OK.\n\n![t5](https://seccdn.unoiou.com/img/articles/tutorial-install-chocolatey/t5.png)\n\n1. Reboot your computer to activate some functionality of `Chocolatey`.\n\nDone!\n\n### Install Python\n\n#### Commands\n\nTry `choco -?` to see its brief introduction. In this case, you will use following commands:\n\n```cmd\nchoco list\nchoco search [software_name]\nchoco install [software_name1, software_name2, ..., software_nameN]\n```\n\nTo list local packages, type `choco list` on PowerShell.\n\nTo search specific software or package, e.g. Python, just type `choco search python`.\n\nTo install packages or softwares, e.g. Python and Anaconda, just type `choco install python3 python2 anaconda`\n\n#### Easy Way\n\nOpen your browser and go to [https://chocolatey.org/packages](https://chocolatey.org/packages).\n\nSearch the package you want.\n\n![t6](https://seccdn.unoiou.com/img/articles/tutorial-install-chocolatey/t6.png)\n\nNow you get the available packages and installation command.\n\n![t7](https://seccdn.unoiou.com/img/articles/tutorial-install-chocolatey/t7.png)\n\nCopy the installation command and paste it to PowerShell window. Click `Enter` to run it.\n"},{"title":"Optimize InfluxDB Data Insertion","url":"/articles/2019/04/18/database/influxdb-insert-optimization/","content":"\n## Issue Description\n\nWe know that InfluxDB can query time series data more efficiently than SQL databases, but there are still some bugs when we insert or query data, they are:\n\n- SQL can return a result eventually even if it costs lots of time, but InfluxDB sometimes does return empty result set.\n- Data in InfluxDB might be lost and we can find it back by `backup & import`.\n- InfluxDB over Nginx or other HTTP server may encounter various problem such as `504 gateway time-out`.\n\nBesides, importing data to InfluxDB is really slow. UDP is fast, but it's not reliable for this case.\n\n## Solutions\n\nTo optimize the process of importing data, we can do these things.\n\n- Add more memory to your server.\n- Set the HTTP server's `time-out` and `max request size` to avoid `gateway timeout` or other HTTP server badcode.\n- Using agent server like Nginx will make double-transfer your data packet, because request data is first sent to Nginx, and Nginx will not transfer your request data until it take over the request body.\n- Do not split your request to batches as possible as you can.\n- Dump your data to import and not use HTTP request.\n\n"},{"title":"Fix WSL Vim Font Issue on Windows 10","url":"/articles/2019/04/18/time-wasting-stuff/fix-windows-wsl-vim-font-issue/","content":"\n## Issue\n\nAfter installed `Vim` on `wsl`, I found that the display font changed to `SimSun`(A very ugly font) when I open Vim.\n\nI tried to use `:set guifont font-name` but it didn't work at all, it annoyed me a lot. The hatred of Windows-fucking-bug-10 had seen absolute growth. :( What the fuck?!\n\n## Solution\n\nFourtunately, I found a solution by editing `Registry` on Windows 10.\n\n- Add a new `DWORD` record in `HKEY_CURRENT_USER\\Console\\C:_Program Files_WindowsApps_CanonicalGroupLimited.UbuntuonWindows_1804.2018.817.0_x64__79rhkp1fndgsc_ubuntu.exe` named `CodePage`\n- Set it to `Decimal` and value `65001` or `Hexadecimal` and value `fde9`\n- Restart the wsl window\n\nIt's fucking working now. Fuck! I do not judge Windows 10, but I've wasted a lot of time solving these sort of egg-painful things!\n\n## Once for All Solution\n\nCommand line on Windows is incurable, anything has done is just a waste of time. The best solution is just use [Cmder](https://cmder.net/) instead.\n\nBesides, I suggest you to use `zsh` instead of `Powershit`(`Powershell`) or `cnm`(`cmd`).\n\nAfter installed `zsh`, `oh-my-zsh` on your `WSL`, append following scripts to your `~/.bashrc` file.\n\n```bash\nbash -c zsh\n```\n\nAnd set `{WSL::bash}` as the default shell of `cmder`.\n\nNow enjoy your new fantastic terminal. :)\n\n"},{"title":"Fix Vim Red Background Color Issue on Windows WSL.","url":"/articles/2019/04/18/tutorial/fix-vim-bg-color-on-windows/","content":"\n## Issue\n\nThe color of blank space which is outside of the view turns red when I move my cursor to them.\n\nResize or reopen the Vim window can temporarily fix it.\n\n## Solution\n\nExport a variable called `TERM` can fix this issue permanently.\n\n```bash\nexport TERM=xterm+256colors\n```\n"},{"title":"Automatically Deploy Your Documents by Continuous Delivery","url":"/articles/2019/04/17/tutorial/articles-continuous-deployment/","content":"\nStatic sites generators such as Hexo, Jekyll, and Hugo etc. have save us from handling CSS/Javascript things. However, we still need to spend a lot of time to configurate generators and build our sites locally then publish/upload it to host.\n\nWith the help of continuous-integration platform, we may let the CI platform do the devil jobs. We just write markdowns, send them to the pipeline, then we are free to go.\n\nSo I started a project called [`Articles`](https://github.com/cmsax/articles), it can automatically build your articles on the cloud with continuous deployment.\n\nYou just concentrate on writing and never install `npm` or `Hexo` on your machine.\n\n`Articles` is enpowered by Hexo, `GitHub pages` and currently `Travis-CI`. You write the articles locally, and we build & publish your articles according your config.\n\nYou can find more information [here](https://github.com/cmsax/articles)\n"},{"title":"Our Solution to Electron Continuous Integration","url":"/articles/2018/07/06/cicd/electron-continuous-integration/","content":"\n## Background\n\nTDD and Auto DevOps recently have become popular for rapid iteration in small team.\n\nThough I am just a freshman in front-end development, I have to choose a suitable technology stack for our coming project called `Shannon`.\n\n## Solution\n\n### Stack\n\n- Electron for building cross-platform desktop app.\n- Electron-Vue as JS framework.\n- Spectron as Electron functional TestSuit.\n- Vue-test-utils as Vue components unit TestSuit.\n- GitLab, GitLab runner as CI-CD envrionment.\n- Kubernetes as backend.\n\n### Steps\n\n- Build a docker image for buiding and testing our app.\n- Create a `.gitlab-ci.yml` file to config our ci-cd tasks.\n- Write our test codes.\n- Write our app.\n- Improve our test code coverage.\n- Iteration.\n\n### Tips\n\nDependencies of a Docker image for running an Electron app:\n\n- Xvfb\n- wine32\n- node.js V8\n- npm or yarn\n\nThe image has been published to docker hub, you can pull it by:\n\n```bash\ndocker pull windworship/npm-image\n```\n\nDo not catch any exceptions in your test code.\n\n"}]